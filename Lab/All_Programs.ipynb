{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d486157",
   "metadata": {},
   "source": [
    "**1. Write a python program to remove punctuations from the given string.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d6935e",
   "metadata": {},
   "source": [
    "How it works:\n",
    "\n",
    "***string.punctuation*** contains all the common punctuation characters (like !\"#$%&'()*+,-./:;<=>?@[\\]^_{|}~).\n",
    "\n",
    "str.maketrans('', '', string.punctuation) creates a translation table to remove them.\n",
    "\n",
    ".translate() uses that table to strip them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15eb6b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a string: Hello! How are You?\n",
      "String without punctuation: Hello How are You\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def remove_punctuation(input_str):\n",
    "    # Create a translation table that maps each punctuation character to None\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    # Use the translate method to remove punctuation\n",
    "    return input_str.translate(translator)\n",
    "\n",
    "# Example usage\n",
    "user_input = input(\"Enter a string: \")\n",
    "cleaned_string = remove_punctuation(user_input)\n",
    "print(\"String without punctuation:\", cleaned_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c195b603",
   "metadata": {},
   "source": [
    "***2. Write a python program to sort the sentence in alphabetical order?***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3da2325d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a sentence: The quick brown fox jumps over the lazy dog\n",
      "Sorted sentence: brown dog fox jumps lazy over quick The the\n"
     ]
    }
   ],
   "source": [
    "def sort_sentence_alphabetically(sentence):\n",
    "    # Split the sentence into words\n",
    "    words = sentence.split()\n",
    "\n",
    "    # Sort the words alphabetically (case-insensitive)\n",
    "    sorted_words = sorted(words, key=lambda word: word.lower())\n",
    "\n",
    "    # Join them back into a sentence\n",
    "    return ' '.join(sorted_words)\n",
    "\n",
    "# Example usage\n",
    "user_input = input(\"Enter a sentence: \")\n",
    "sorted_sentence = sort_sentence_alphabetically(user_input)\n",
    "print(\"Sorted sentence:\", sorted_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd4a396",
   "metadata": {},
   "source": [
    "***3. Write apython program to remove stop words for a given passage using NLTK.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e6645a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a passage: The quick brown fox jumps over the lazy dog\n",
      "List of Stopwords: -------- {\"couldn't\", 'wouldn', 'theirs', \"i'll\", 'at', 'during', \"it's\", \"she's\", 'or', 'doesn', 'hasn', \"isn't\", 'where', 'didn', 'our', 'them', 'mightn', 'shan', 'll', \"you're\", \"they've\", 'with', 'will', 'that', \"mightn't\", 'only', 'before', 'of', 'yours', 'be', \"he's\", 'd', \"we'll\", 'few', \"i'm\", 'ourselves', 'he', 'are', 'through', 't', 's', \"he'd\", 'if', \"she'll\", 'don', \"i'd\", 'against', \"doesn't\", 'had', \"needn't\", 'we', \"hasn't\", 'needn', 'yourselves', 'been', 'themselves', 'why', 'am', 'its', 'after', 'too', 'out', 'very', 'under', 'while', 'herself', 'both', 'her', 'each', \"it'd\", 'and', 'to', 'for', 'who', 'again', 'haven', \"aren't\", 'ain', 'when', 'into', \"that'll\", \"mustn't\", \"we're\", 'most', 'more', 'does', 'there', 'y', 'mustn', 'him', 'his', 'above', 'over', 'just', 'were', 'do', \"they'll\", 'any', \"he'll\", 'ours', 'until', \"haven't\", 'from', 'she', 'should', 'can', 'yourself', \"shan't\", 'you', 'what', \"you've\", 'o', 're', \"they're\", \"you'd\", 'shouldn', 'himself', 'how', 'itself', 'the', 'hadn', 'was', 'below', 'as', \"hadn't\", 'once', 'this', 'couldn', 'i', 'off', \"shouldn't\", 'about', \"wouldn't\", 'in', \"we've\", 'some', \"you'll\", 'nor', 'your', 'all', \"should've\", 'not', 'having', \"it'll\", 'my', 'ma', 'up', 'now', \"don't\", 'myself', 'being', \"wasn't\", 'other', 'is', 'whom', 'an', \"we'd\", 'have', \"weren't\", 'here', 'by', 'did', 'm', 'hers', 'so', 'they', 'own', 'these', 'such', 'their', 'further', 'on', 'has', 'doing', 'me', 'because', \"they'd\", 'it', 'wasn', 've', \"i've\", 'aren', 'than', 'then', 'but', 'isn', 'same', 'no', \"she'd\", \"won't\", 'between', 'those', 'which', 'won', 'down', 'a', \"didn't\", 'weren'}\n",
      "\n",
      "Passage without stop words:\n",
      "quick brown fox jumps lazy dog\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download required NLTK data (only the first time)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def remove_stopwords(passage):\n",
    "    # Tokenize the passage into words\n",
    "    words = word_tokenize(passage)\n",
    "\n",
    "    # Get English stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    print(\"List of Stopwords: --------\", stop_words)\n",
    "    # Filter out stopwords\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Example usage\n",
    "user_input = input(\"Enter a passage: \")\n",
    "cleaned_passage = remove_stopwords(user_input)\n",
    "print(\"\\nPassage without stop words:\")\n",
    "print(cleaned_passage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e42dd8",
   "metadata": {},
   "source": [
    "***4. Write a python program to implement stemming for a given sentence using NLTK***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a18cad4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence:\n",
      "The striped bats were hanging on their feet and eating best fruits quickly\n",
      "\n",
      "Stemmed Words:\n",
      "the stripe bat were hang on their feet and eat best fruit quickli\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download required NLTK data (only once)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Initialize the stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Input sentence\n",
    "sentence = \"The striped bats were hanging on their feet and eating best fruits quickly\"\n",
    "\n",
    "# Tokenize the sentence into words\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "# Apply stemming\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "# Print original and stemmed words\n",
    "print(\"Original Sentence:\")\n",
    "print(sentence)\n",
    "\n",
    "print(\"\\nStemmed Words:\")\n",
    "print(\" \".join(stemmed_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce005e2",
   "metadata": {},
   "source": [
    "***5. Write a python program to implement Lemmatization using NLTK***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb04c27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence:\n",
      "The striped bats were hanging on their feet and eating best fruits quickly\n",
      "\n",
      "Lemmatized Words:\n",
      "The striped bat were hanging on their foot and eating best fruit quickly\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download required resources (only needed once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')  # Optional: for better lemmatization\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Input sentence\n",
    "sentence = \"The striped bats were hanging on their feet and eating best fruits quickly\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "# Apply lemmatization (default POS is 'n' for noun)\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "# Print original and lemmatized sentence\n",
    "print(\"Original Sentence:\")\n",
    "print(sentence)\n",
    "\n",
    "print(\"\\nLemmatized Words:\")\n",
    "print(\" \".join(lemmatized_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d8b924",
   "metadata": {},
   "source": [
    "***6. Write a python program to POS (Parts of Speech) tagging for the give sentence using NLTK***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ba9cb77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tagged Sentence:\n",
      "The -> DT\n",
      "quick -> JJ\n",
      "brown -> NN\n",
      "fox -> NN\n",
      "jumps -> VBZ\n",
      "over -> IN\n",
      "the -> DT\n",
      "lazy -> JJ\n",
      "dog -> NN\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download required NLTK resources (only once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Input sentence\n",
    "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "# Perform POS tagging\n",
    "pos_tags = nltk.pos_tag(words)\n",
    "\n",
    "# Print the POS tagged words\n",
    "print(\"POS Tagged Sentence:\")\n",
    "for word, tag in pos_tags:\n",
    "    print(f\"{word} -> {tag}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8338c419",
   "metadata": {},
   "source": [
    "***Common POS Tags:*** \\\n",
    "Tag\tMeaning\\ \n",
    "NN\tNoun, singular \\ \n",
    "NNS\tNoun, plural \\\n",
    "VB\tVerb, base form \\ \n",
    "VBD\tVerb, past tense \\ \n",
    "VBG\tVerb, gerund \\\n",
    "VBN\tVerb, past participle \\ \n",
    "VBP\tVerb, non-3rd person \\\n",
    "VBZ\tVerb, 3rd person \\\n",
    "JJ\tAdjective \\\n",
    "RB\tAdverb \\\n",
    "DT\tDeterminer \\ \n",
    "IN\tPreposition \\ \n",
    "PRP\tPersonal pronoun \\ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5a75d6",
   "metadata": {},
   "source": [
    "***7. Write a python program to for Text Classification for the give sentence using NLTK***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d4a3b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
